from typing import Dict, List
from openai import OpenAI
import os
from dotenv import load_dotenv
from pathlib import Path

# Get the absolute path to the src directory
SRC_DIR = Path(__file__).parent.parent
UTILS_DIR = SRC_DIR / "utils"

# Import using file path
import sys
sys.path.append(str(SRC_DIR))
from utils.prompt_templates import PromptTemplates

load_dotenv()

class GPTClient:
    def __init__(self, api_key: str = None, model: str = None):
        """
        Initialize GPT client for novel generation with enhanced chat history support
        
        Args:
            api_key: OpenAI API key
            model: Model name to use (default from env or gpt-4)
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API key is required")
        
        # Initialize OpenAI client with new API
        self.client = OpenAI(api_key=self.api_key)
        
        # Model configuration from environment or default
        self.model = model or os.getenv("OPENAI_MODEL", "gpt-4")
        self.max_tokens = int(os.getenv("MAX_TOKENS", "1500"))
        self.temperature = float(os.getenv("TEMPERATURE", "0.7"))
        
        # Chat history management settings
        self.max_history_tokens = int(os.getenv("MAX_HISTORY_TOKENS", "3000"))  # íˆìŠ¤í† ë¦¬ í† í° í•œë„
        self.max_context_length = int(os.getenv("MAX_CONTEXT_LENGTH", "6000"))  # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ í•œë„

    def _estimate_tokens(self, text: str) -> int:
        """
        Estimate token count for text (rough estimation: 1 token â‰ˆ 3.5 characters for Korean/English mix)
        """
        return max(1, len(text) // 3)

    def _prepare_chat_history(self, messages: List[Dict], current_user_message: str, system_prompt: str) -> List[Dict]:
        """
        Prepare chat history while staying within token limits
        ìµœëŒ€í•œ ë§ì€ íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ í† í° í•œë„ë¥¼ ì¤€ìˆ˜
        
        Args:
            messages: ì±„íŒ… íˆìŠ¤í† ë¦¬
            current_user_message: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            
        Returns:
            í† í° í•œë„ ë‚´ì˜ ìµœì í™”ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        """
        if not messages:
            return []
        
        # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
        system_tokens = self._estimate_tokens(system_prompt)
        current_tokens = self._estimate_tokens(current_user_message)
        available_tokens = self.max_context_length - self.max_tokens - system_tokens - current_tokens - 500  # ì—¬ìœ ë¶„
        
        chat_history = []
        total_tokens = 0
        
        # ìµœì‹  ë©”ì‹œì§€ë¶€í„° ì—­ìˆœìœ¼ë¡œ ì¶”ê°€ (ìµœì‹  ì»¨í…ìŠ¤íŠ¸ ìš°ì„  ë³´ì¡´)
        for msg in reversed(messages):
            msg_tokens = 0
            temp_messages = []
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ì™€ AI ì‘ë‹µì„ ì„¸íŠ¸ë¡œ ì²˜ë¦¬
            if "User" in msg and "LLM_Model" in msg:
                user_content = msg["User"]
                assistant_content = msg["LLM_Model"]
                
                user_tokens = self._estimate_tokens(user_content)
                assistant_tokens = self._estimate_tokens(assistant_content)
                msg_tokens = user_tokens + assistant_tokens
                
                if total_tokens + msg_tokens <= available_tokens:
                    temp_messages = [
                        {"role": "user", "content": user_content},
                        {"role": "assistant", "content": assistant_content}
                    ]
                else:
                    # í† í° í•œë„ ì´ˆê³¼ì‹œ ì¤‘ë‹¨
                    break
                    
            elif "User" in msg:
                user_content = msg["User"]
                msg_tokens = self._estimate_tokens(user_content)
                
                if total_tokens + msg_tokens <= available_tokens:
                    temp_messages = [{"role": "user", "content": user_content}]
                else:
                    break
                    
            elif "LLM_Model" in msg:
                assistant_content = msg["LLM_Model"]
                msg_tokens = self._estimate_tokens(assistant_content)
                
                if total_tokens + msg_tokens <= available_tokens:
                    temp_messages = [{"role": "assistant", "content": assistant_content}]
                else:
                    break
            
            # ë©”ì‹œì§€ ì¶”ê°€ (ì•ìª½ì— ì‚½ì…í•˜ì—¬ ì‹œê°„ìˆœ ìœ ì§€)
            for temp_msg in reversed(temp_messages):
                chat_history.insert(0, temp_msg)
            total_tokens += msg_tokens
        
        return chat_history

    def chat_session(self, chapter_num: str, context: Dict, user_message: str, messages: List[Dict] = None) -> str:
        """
        Generate novel content using comprehensive chat history and optimized prompts
        
        Args:
            chapter_num: í˜„ì¬ ì±•í„° ë²ˆí˜¸
            context: ì†Œì„¤ ì»¨í…ìŠ¤íŠ¸ (ì´ì „ ì±•í„°, ì±… ì •ë³´ ë“±)
            user_message: ì‚¬ìš©ì ì…ë ¥
            messages: ì±„íŒ… íˆìŠ¤í† ë¦¬ (ìµœëŒ€í•œ ë§ì´ í™œìš©)
            
        Returns:
            ìƒì„±ëœ ì†Œì„¤ ë‚´ìš©
        """
        try:
            # PromptTemplatesì—ì„œ ì „ë¬¸ì ì¸ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            system_prompt = PromptTemplates.get_chapter_prompt(chapter_num, context)
            
            # ë©”ì‹œì§€ êµ¬ì„± ì‹œì‘
            chat_messages = [{"role": "system", "content": system_prompt}]
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìµœëŒ€í•œ ë§ì´ ì¶”ê°€ (í† í° í•œë„ ë‚´ì—ì„œ)
            if messages:
                history_messages = self._prepare_chat_history(messages, user_message, system_prompt)
                chat_messages.extend(history_messages)
                print(f"ğŸ“š Chat history: {len(history_messages)} messages loaded")
            
            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            chat_messages.append({"role": "user", "content": user_message})
            
            # ì†Œì„¤ ìƒì„±ì— ìµœì í™”ëœ íŒŒë¼ë¯¸í„°ë¡œ ìš”ì²­
            response = self.client.chat.completions.create(
                model=self.model,
                messages=chat_messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens,
                presence_penalty=0.3,   # ìƒˆë¡œìš´ ì•„ì´ë””ì–´ ìƒì„± ì¥ë ¤
                frequency_penalty=0.2,  # ë°˜ë³µ ê°ì†Œ
                top_p=0.95,            # ì°½ì˜ì„±ì„ ìœ„í•œ nucleus sampling
                stream=False
            )
            
            generated_content = response.choices[0].message.content
            print(f"âœ… Generated {len(generated_content)} characters")
            return generated_content
            
        except Exception as e:
            raise Exception(f"Error generating novel content: {str(e)}")

    def summarize_chapter(self, content: str, chapter_num: str = None) -> str:
        """
        Generate comprehensive chapter summary using enhanced prompt template
        
        Args:
            content: ì±•í„° ë‚´ìš©
            chapter_num: ì±•í„° ë²ˆí˜¸ (ì„ íƒì‚¬í•­)
            
        Returns:
            ì „ë¬¸ì ì¸ ì±•í„° ìš”ì•½
        """
        try:
            # PromptTemplatesì—ì„œ ì „ë¬¸ì ì¸ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            system_prompt = PromptTemplates.get_summary_prompt(content)
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt}
                ],
                temperature=0.3,  # ìš”ì•½ì€ ì¼ê´€ì„±ì´ ì¤‘ìš”
                max_tokens=400,   # ë” ìƒì„¸í•œ ìš”ì•½ì„ ìœ„í•´ ì¦ê°€
                top_p=0.8
            )
            
            summary = response.choices[0].message.content
            print(f"ğŸ“ Chapter summary generated: {len(summary)} characters")
            return summary
            
        except Exception as e:
            raise Exception(f"Error summarizing chapter: {str(e)}")

# if __name__ == "__main__":
#     try:
#         print("\n=== GPT Client Test ===")
        
#         # Initialize GPT client with GPT-3.5-turbo model
#         gpt_client = GPTClient(model="gpt-4o")
#         print("âœ… GPT Client initialized successfully")
        
#         # Test context for novel generation
#         test_context = {
#             "title": "ë§ˆë²•ì‚¬ì˜ ì—¬í–‰",
#             "genre": "íŒíƒ€ì§€",
#             "main_character": "ì Šì€ ë§ˆë²•ì‚¬ ì•„ë¦°",
#             "setting": "ì¤‘ì„¸ íŒíƒ€ì§€ ì„¸ê³„",
#             "previous_chapters": [
#                 "ì•„ë¦°ì€ ë§ˆë²• í•™êµë¥¼ ì¡¸ì—…í•˜ê³  ì²˜ìŒìœ¼ë¡œ ëª¨í—˜ì„ ë– ë‚˜ê¸°ë¡œ í–ˆë‹¤.",
#                 "ê·¸ë…€ëŠ” ì „ì„¤ì˜ ë§ˆë²• ìœ ë¬¼ì„ ì°¾ì•„ ë– ë‚˜ëŠ” ì—¬ì •ì„ ì‹œì‘í–ˆë‹¤."
#             ]
#         }
        
#         print("\n1. Testing Novel Generation")
#         print("-" * 50)
#         # Test chapter generation
#         test_messages = [
#             {
#                 "User": "ì•„ë¦°ì´ ì²« ë²ˆì§¸ ë§ˆì„ì— ë„ì°©í•´ì„œ ê²ªëŠ” ì´ì•¼ê¸°ë¥¼ ì¨ì¤˜.",
#                 "LLM_Model": "ì•„ë¦°ì€ í•´ì§ˆë…˜ì— ì‘ì€ ë§ˆì„ 'ë¸”ë£¨ë°ë°ì¼'ì— ë„ì°©í–ˆë‹¤..."
#             }
#         ]
        
#         test_user_message = "ì•„ë¦°ì´ ë§ˆì„ì—ì„œ ì‹ ë¹„í•œ ìƒì ì„ ë°œê²¬í•˜ê³ ..."
#         chapter_content = gpt_client.chat_session(
#             chapter_num="3",
#             context=test_context,
#             user_message=test_user_message,
#             messages=test_messages
#         )
#         print("\nGenerated Chapter Content:")
#         print("-" * 50)
#         print(chapter_content)
#         print("-" * 50)
        
#         print("\n2. Testing Chapter Summary")
#         print("-" * 50)
#         # Test summary generation
#         summary = gpt_client.summarize_chapter(chapter_content, "3")
#         print("\nChapter Summary:")
#         print("-" * 50)
#         print(summary)
#         print("-" * 50)
        
#         print("\nâœ… All tests completed successfully!")
        
#     except Exception as e:
#         print(f"\nâŒ Test failed: {str(e)}")
#         raise e